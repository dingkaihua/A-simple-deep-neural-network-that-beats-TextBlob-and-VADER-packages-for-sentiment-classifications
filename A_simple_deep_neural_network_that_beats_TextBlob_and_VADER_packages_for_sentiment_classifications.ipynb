{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A simple deep neural network that beats TextBlob and VADER packages for sentiment classifications.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceI3oXiGrD74"
      },
      "source": [
        "This is the accompanying IPython notebook for the medium article **A simple deep neural network that beats TextBlob and VADER packages for sentiment classifications, written in Python.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-2w7ACErQ_3"
      },
      "source": [
        "There are explainations for every code block. More detailed explaination can be found in the original Medium article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOTS3tOftcOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e574d00-cea8-47c0-d529-d74919f301ba"
      },
      "source": [
        "import os \n",
        "import random as rnd\n",
        "import re\n",
        "import string\n",
        "\n",
        "# import relevant libraries\n",
        "!pip install -q -U trax\n",
        "import trax\n",
        "\n",
        "# import trax.fastmath.numpy\n",
        "import trax.fastmath.numpy as np # the same a Jax\n",
        "from trax import fastmath\n",
        "\n",
        "# import trax.layers\n",
        "from trax import layers as tl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 637 kB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 55.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 13 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 54.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 49.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 66.3 MB/s \n",
            "\u001b[?25h  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIUMCV5HztH9"
      },
      "source": [
        "# Construct a vocabulary dictionary, this is a simple model, to convert words into numerical vectors. nltk Twitter sample will be used to contruct the vocabulary dictionary in this case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN7ZOWVL0YUB"
      },
      "source": [
        "### Download labelled nltk twitter samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H81yvB2rwXa6",
        "outputId": "659a9bf7-031a-4af4-9ea9-40f19379f28d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords, twitter_samples \n",
        "from nltk.tokenize import TweetTokenizer\n",
        "stopwords_english = stopwords.words('english')\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6VURRil0c56"
      },
      "source": [
        "### Helper functions to clean twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W81ufm18wewr"
      },
      "source": [
        "def load_tweets():\n",
        "    all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "    all_negative_tweets = twitter_samples.strings('negative_tweets.json')  \n",
        "    return all_positive_tweets, all_negative_tweets\n",
        "    \n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    \n",
        "    '''\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    ### START CODE HERE ###\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and # remove stopwords\n",
        "            word not in string.punctuation): # remove punctuation\n",
        "            #tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word) # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "    ### END CODE HERE ###\n",
        "    return tweets_clean\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGNg3uYe1gIN"
      },
      "source": [
        "### Construct vocabulary dictionary using both the downloaded data and the helper funciton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-VNXp7Gwm4L"
      },
      "source": [
        "# Load positive and negative tweets\n",
        "all_positive_tweets, all_negative_tweets = load_tweets()\n",
        "\n",
        "train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
        "train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
        "\n",
        "# Combine training data into one set\n",
        "train_x = train_pos + train_neg \n",
        "\n",
        "# Build the vocabulary\n",
        "# Unit Test Note - There is no test set here only train/val\n",
        "\n",
        "# Include special tokens \n",
        "# started with pad, end of line and unk tokens\n",
        "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
        "\n",
        "# Note that we build vocab using training data\n",
        "for tweet in train_x: \n",
        "    processed_tweet = process_tweet(tweet)\n",
        "    for word in processed_tweet:\n",
        "        if word not in Vocab: \n",
        "            Vocab[word] = len(Vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB3W7Vll0uTB"
      },
      "source": [
        "# Model description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1eU9yYJuPk_"
      },
      "source": [
        "def classifier(vocab_size=10000, embedding_dim=256, output_dim=2, mode='train'):\n",
        "    # create embedding layer\n",
        "    embed_layer = tl.Embedding(\n",
        "        vocab_size=vocab_size, # Size of the vocabulary\n",
        "        d_feature=embedding_dim)  # Embedding dimension\n",
        "    \n",
        "    # Create a mean layer, to create an \"average\" word embedding\n",
        "    mean_layer = tl.Mean(axis=1)\n",
        "    \n",
        "    # Create a dense layer, one unit for each output\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
        "\n",
        "    # Create the log softmax layer (no parameters needed)\n",
        "    log_softmax_layer = tl.LogSoftmax()\n",
        "    \n",
        "    # Use tl.Serial combinator\n",
        "    model = tl.Serial(\n",
        "      embed_layer, # embedding layer\n",
        "      mean_layer, # mean layer\n",
        "      dense_output_layer, # dense output layer \n",
        "      log_softmax_layer # log softmax layer\n",
        "    )\n",
        "    \n",
        "    # return the model of type\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxii-6D-2LQj"
      },
      "source": [
        "### I will provide the trained weights of this model. The details of the model is as below. You are welcome to re-train this model to fit your needs if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "-ZHPVL5R2IKa",
        "outputId": "99042657-ea69-40c6-83c1-a0b31464c937"
      },
      "source": [
        "model = classifier()\n",
        "display(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Serial[\n",
              "  Embedding_10000_256\n",
              "  Mean\n",
              "  Dense_2\n",
              "  LogSoftmax\n",
              "]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHMtOEso0yB9"
      },
      "source": [
        "### Helper function for both training and predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXY6UV2zv5Nt"
      },
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet - A string containing a tweet\n",
        "        vocab_dict - The words dictionary\n",
        "        unk_token - The special string for unknown tokens\n",
        "        verbose - Print info durign runtime\n",
        "    Output:\n",
        "        tensor_l - A python list with\n",
        "        \n",
        "    '''  \n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # Process the tweet into a list of words\n",
        "    # where only important words are kept (stop words removed)\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"List of words from the processed tweet:\")\n",
        "        print(word_l)\n",
        "        \n",
        "    # Initialize the list that will contain the unique integer IDs of each word\n",
        "    tensor_l = []\n",
        "    \n",
        "    # Get the unique integer ID of the __UNK__ token\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
        "        \n",
        "    # for each word in the list:\n",
        "    for word in word_l:\n",
        "        \n",
        "        # Get the unique integer ID.\n",
        "        # If the word doesn't exist in the vocab dictionary,\n",
        "        # use the unique ID for __UNK__ instead.\n",
        "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "        # Append the unique integer ID to the tensor list.\n",
        "        tensor_l.append(word_ID) \n",
        "    \n",
        "    return tensor_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDHDlc-x2eHk",
        "outputId": "b4dec639-5737-4588-da88-4f38f4967ff1"
      },
      "source": [
        "help(model.init_from_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method init_from_file in module trax.layers.base:\n",
            "\n",
            "init_from_file(file_name, weights_only=False, input_signature=None) method of trax.layers.combinators.Serial instance\n",
            "    Initializes this layer and its sublayers from a pickled checkpoint.\n",
            "    \n",
            "    In the common case (`weights_only=False`), the file must be a gziped pickled\n",
            "    dictionary containing items with keys `'flat_weights', `'flat_state'` and\n",
            "    `'input_signature'`, which are used to initialize this layer.\n",
            "    If `input_signature` is specified, it's used instead of the one in the file.\n",
            "    If `weights_only` is `True`, the dictionary does not need to have the\n",
            "    `'flat_state'` item and the state it not restored either.\n",
            "    \n",
            "    Args:\n",
            "      file_name: Name/path of the pickled weights/state file.\n",
            "      weights_only: If `True`, initialize only the layer's weights. Else\n",
            "          initialize both weights and state.\n",
            "      input_signature: Input signature to be used instead of the one from file.\n",
            "    \n",
            "    Returns:\n",
            "      A `(weights, state)` tuple.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jdkAVsnrmrF"
      },
      "source": [
        "The path below should be where you saved the pre-trained weights. I have pre-trained this DNN and you can find my saved weights here, https://github.com/dingkaihua/A-simple-deep-neural-network-that-beats-TextBlob-and-VADER-packages-for-sentiment-classifications/blob/main/checkpoints/model.pkl.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjRAWBktssXg"
      },
      "source": [
        "Simply download the model.pkl.gz and save it somewhere. Then, give model.pkl.gz's path to PATH variable below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thEPrZj_uaZN"
      },
      "source": [
        "PATH = YOUR_OWN_PATH # for me, it was '/content/drive/MyDrive/Colab_Notebooks/Medium/checkpoints/model.pkl.gz'\n",
        "weights, state = model.init_from_file(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t89EDArR2sd1"
      },
      "source": [
        "### Helper function to call model directly for prediciton"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPmD1EKavwsO"
      },
      "source": [
        "def predict(sentence):\n",
        "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
        "    \n",
        "    # Batch size 1, add dimension for batch, to work with the model\n",
        "    inputs = inputs[None, :]  \n",
        "    \n",
        "    # predict with the model\n",
        "    preds_probs = model(inputs) # log softmax result\n",
        "    \n",
        "    # Turn probabilities into categories\n",
        "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "    \n",
        "    sentiment = \"negative\"\n",
        "    if preds == 1:\n",
        "        sentiment = 'positive'\n",
        "\n",
        "    return preds, sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wTGpHfh201P"
      },
      "source": [
        "# Test: classification accuracy comparison among TextBlob, VADER and a regular deep neural networks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJPFLmvK3Ml3",
        "outputId": "a440f75a-e059-4f6c-fef5-27117282d624"
      },
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "!pip3 install -U nltk[twitter] \n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer # ask VADER to use Twitter lexicon for fairness of comparison"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "Requirement already satisfied: nltk[twitter] in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk[twitter]) (1.15.0)\n",
            "Collecting twython\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython->nltk[twitter]) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython->nltk[twitter]) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython->nltk[twitter]) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython->nltk[twitter]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython->nltk[twitter]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython->nltk[twitter]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython->nltk[twitter]) (3.1.1)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ4IzbdL2-gm"
      },
      "source": [
        "# For convinence, I wote this wrapper to call both TextBlob and VADER\n",
        "def analize_sentiment(sentence, option='VADER'):\n",
        "    '''\n",
        "    Utility function to classify the polarity of a tweet\n",
        "    using textblob.\n",
        "    '''\n",
        "\n",
        "    if option == 'VADER':\n",
        "      analysis = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
        "      analysis = analysis['compound'] # take the compound score\n",
        "    elif option == \"TextBlob\":\n",
        "      analysis = TextBlob(tweet)\n",
        "      analysis = analysis.sentiment.polarity\n",
        "\n",
        "    if analysis > 0:\n",
        "        return \"positive\"\n",
        "    elif analysis == 0:\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return \"negative\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO-fz1lL3fgi"
      },
      "source": [
        "### Kai hand engineered the following tests set, consist of negation, double negations and use of idioms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo1oiapg3eqR"
      },
      "source": [
        "hand_engineered_tests= [[\"The movie is almost good.\", \"negative\"], # negation\n",
        "                       [\"I don't think the movie is good.\", \"negative\"], # negation\n",
        "                       [\"This movie is ridiculously underrated. This movie can not be any better!\", \"positive\"], # double negation\n",
        "                       [\"The movie is not bad.\", \"positive\"], # double negation\n",
        "                       [\"I can't believe how bad this movie is.\", \"negative\"], # expression that contains negation as exclaimation\n",
        "                       [\"I can't believe how great this movie is.\", \"positive\"], # expression that contains negation as exclaimation\n",
        "                       [\"This movie is ridiculously great.\", \"positive\"],\n",
        "                       [\"This movie is ridiculously horrible.\", \"negative\"],\n",
        "                       [\"One of the worst film that I have seen in my life.\", \"negative\"],\n",
        "                       [\"This is about as entertaining as watching paint dry.\", \"negative\"],  # usage of idiom\n",
        "                       [\"Not a good choice.\", \"negative\"],\n",
        "                       [\"Wasted 2 hours of my life on this moive that I can never get back.\", \"negative\"]] # scarsm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbNVSov9izA9",
        "outputId": "38335626-b883-4f1c-956d-6d7b6a9ec64a"
      },
      "source": [
        "from tabulate import tabulate\n",
        "print(tabulate(hand_engineered_tests, headers=['Hand engineered tweet', 'Sentiment'], tablefmt='orgtbl'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Hand engineered tweet                                                    | Sentiment   |\n",
            "|--------------------------------------------------------------------------+-------------|\n",
            "| The movie is almost good.                                                | negative    |\n",
            "| I don't think the movie is good.                                         | negative    |\n",
            "| This movie is ridiculously underrated. This movie can not be any better! | positive    |\n",
            "| The movie is not bad.                                                    | positive    |\n",
            "| I can't believe how bad this movie is.                                   | negative    |\n",
            "| I can't believe how great this movie is.                                 | positive    |\n",
            "| This movie is ridiculously great.                                        | positive    |\n",
            "| This movie is ridiculously horrible.                                     | negative    |\n",
            "| One of the worst film that I have seen in my life.                       | negative    |\n",
            "| This is about as entertaining as watching paint dry.                     | negative    |\n",
            "| Not a good choice.                                                       | negative    |\n",
            "| Wasted 2 hours of my life on this moive that I can never get back.       | negative    |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt_7KrXT5KGE",
        "outputId": "9f6caefe-6f3d-4fb9-e172-a4fce36f1406"
      },
      "source": [
        "no_hand_engineered_tests = len(hand_engineered_tests)\n",
        "\n",
        "no_correct_classification_TextBlob = 0\n",
        "no_correct_classification_VADER = 0\n",
        "no_correct_classification_deep_neural_nets = 0\n",
        "\n",
        "\n",
        "for i in range(no_hand_engineered_tests):\n",
        "\n",
        "    sample = hand_engineered_tests[i]\n",
        "    sentence = sample[0]\n",
        "    sentiment = sample[1]\n",
        "\n",
        "    # TextBlob\n",
        "    if analize_sentiment(sentence, option='TextBlob') == sentiment:\n",
        "      no_correct_classification_TextBlob+=1\n",
        "\n",
        "    # VADER\n",
        "    if analize_sentiment(sentence, option='VADER') == sentiment:\n",
        "      no_correct_classification_VADER+=1\n",
        "\n",
        "    # deep neural nets\n",
        "    if predict(sentence)[1] == sentiment:\n",
        "      no_correct_classification_deep_neural_nets +=1\n",
        "\n",
        "    \n",
        "print(f\"TextBlob classified {no_correct_classification_TextBlob} / {no_hand_engineered_tests} correctly. \\n\")\n",
        "print(f\"VADER classified {no_correct_classification_VADER} / {no_hand_engineered_tests} correctly. \\n\")\n",
        "print(f\"The deep neural net classified {no_correct_classification_deep_neural_nets} / {no_hand_engineered_tests} correctly.\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob classified 8 / 12 correctly. \n",
            "\n",
            "VADER classified 6 / 12 correctly. \n",
            "\n",
            "The deep neural nets classified 10 / 12 correctly.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nhyCuPXvUqc",
        "outputId": "19885056-79b3-4c3b-8aa5-aed478a0281e"
      },
      "source": [
        "# try a negative sentence\n",
        "sentence = \"I can not believe how fantastic this movie was.\"\n",
        "pred, sentiment = predict(sentence)\n",
        "print(f\"The deep neural net classifies sentiment of the sentence: '{sentence}', to be {sentiment}.\")\n",
        "print(f\"TextBlot classifies sentiment of the sentence: '{sentence}', to be {analize_sentiment(sentence, option='TextBlob')}.\")\n",
        "print(f\"VADER classifies sentiment of the sentence: '{sentence}', to be {analize_sentiment(sentence, option='VADER')}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The deep neural net classifies sentiment of the sentence: 'I can not believe how fantastic this movie was.', to be positive.\n",
            "TextBlot classifies sentiment of the sentence: 'I can not believe how fantastic this movie was.', to be negative.\n",
            "VADER classifies sentiment of the sentence: 'I can not believe how fantastic this movie was.', to be negative.\n"
          ]
        }
      ]
    }
  ]
}